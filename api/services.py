import numpy as np
import pandas as pd
import joblib
import json
import time
import tensorflow as tf
from typing import List, Dict, Any, Tuple
from prophet.serialize import model_from_json
from fastapi import HTTPException
import sklearn.ensemble
import functools
import os
from concurrent.futures import ThreadPoolExecutor, as_completed
import logging
from datetime import datetime, timedelta

from .config import AVAILABLE_MODELS, BASE_DIR
from .schemas import (
    PredictionRequest, SimulationRequest, WeatherConditions,
    CalendarConditions, TimeScenario, EnergyConditions, ZoneConsumption,
    LagOverrides, VolatilityScenario, CompareRequest, ScenarioResult, CompareResponse
)
from .features import generate_simple_features, generate_full_features, create_sequences_for_dl
from .evaluation import evaluate_model

# ============================================================================
# TensorFlow Optimizations
# ============================================================================
# Enable XLA JIT compilation for 2-3x speedup on DL models
tf.config.optimizer.set_jit(True)

# Configure TensorFlow to use available threads efficiently
tf.config.threading.set_intra_op_parallelism_threads(4)
tf.config.threading.set_inter_op_parallelism_threads(2)

logger = logging.getLogger(__name__)
logger.info("TensorFlow optimizations enabled: XLA JIT=True")


def _apply_weather_conditions(X_future: pd.DataFrame, weather: WeatherConditions) -> pd.DataFrame:
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –ø–æ–≥–æ–¥–Ω—ñ —É–º–æ–≤–∏ –¥–æ DataFrame –∑ –æ–∑–Ω–∞–∫–∞–º–∏.
    –ü–æ–≤–µ—Ä—Ç–∞—î –æ–Ω–æ–≤–ª–µ–Ω–∏–π DataFrame.
    """
    weather_mapping = {
        'temperature': weather.temperature,
        'humidity': weather.humidity,
        'wind_speed': weather.wind_speed,
    }

    applied_features = []
    for feature_name, value in weather_mapping.items():
        if value is not None and feature_name in X_future.columns:
            X_future[feature_name] = value
            applied_features.append(f"{feature_name}={value}")

    if applied_features:
        logger.info(f"Applied weather conditions: {', '.join(applied_features)}")

    return X_future


def _apply_calendar_conditions(X_future: pd.DataFrame, calendar: CalendarConditions) -> pd.DataFrame:
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω—ñ —É–º–æ–≤–∏ –¥–æ DataFrame –∑ –æ–∑–Ω–∞–∫–∞–º–∏.
    –ü–æ–≤–µ—Ä—Ç–∞—î –æ–Ω–æ–≤–ª–µ–Ω–∏–π DataFrame.
    """
    calendar_mapping = {
        'is_holiday': 1 if calendar.is_holiday else 0 if calendar.is_holiday is not None else None,
        'is_weekend': 1 if calendar.is_weekend else 0 if calendar.is_weekend is not None else None,
    }

    applied_features = []
    for feature_name, value in calendar_mapping.items():
        if value is not None and feature_name in X_future.columns:
            X_future[feature_name] = value
            applied_features.append(f"{feature_name}={value}")

    if applied_features:
        logger.info(f"Applied calendar conditions: {', '.join(applied_features)}")

    return X_future


def _apply_time_scenario(X_future: pd.DataFrame, time_scenario: TimeScenario) -> pd.DataFrame:
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î —á–∞—Å–æ–≤—ñ —Å—Ü–µ–Ω–∞—Ä—ñ—ó –¥–æ DataFrame –∑ –æ–∑–Ω–∞–∫–∞–º–∏.
    –î–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ –ø—ñ–∫–æ–≤—ñ –≥–æ–¥–∏–Ω–∏ —Ç–∞ —Å–µ–∑–æ–Ω–Ω—ñ—Å—Ç—å.
    """
    time_mapping = {
        'hour': time_scenario.hour,
        'day_of_week': time_scenario.day_of_week,
        'day_of_month': time_scenario.day_of_month,
        'day_of_year': time_scenario.day_of_year,
        'week_of_year': time_scenario.week_of_year,
        'month': time_scenario.month,
        'year': time_scenario.year,
        'quarter': time_scenario.quarter,
    }

    applied_features = []
    for feature_name, value in time_mapping.items():
        if value is not None and feature_name in X_future.columns:
            X_future[feature_name] = value
            applied_features.append(f"{feature_name}={value}")

    if applied_features:
        logger.info(f"Applied time scenario: {', '.join(applied_features)}")

    return X_future


def _apply_zone_consumption(X_future: pd.DataFrame, zone: ZoneConsumption) -> pd.DataFrame:
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –∑–æ–Ω–æ–≤–µ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è –¥–æ DataFrame –∑ –æ–∑–Ω–∞–∫–∞–º–∏.
    –î–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ —Å—Ü–µ–Ω–∞—Ä—ñ—ó –ø–æ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è—Ö –ø—Ä–∏–ª–∞–¥—ñ–≤.
    """
    zone_mapping = {
        'Sub_metering_1': zone.sub_metering_1,
        'Sub_metering_2': zone.sub_metering_2,
        'Sub_metering_3': zone.sub_metering_3,
    }

    applied_features = []
    for feature_name, value in zone_mapping.items():
        if value is not None and feature_name in X_future.columns:
            X_future[feature_name] = value
            applied_features.append(f"{feature_name}={value}")

    if applied_features:
        logger.info(f"Applied zone consumption: {', '.join(applied_features)}")

    return X_future


def _apply_anomaly_flag(X_future: pd.DataFrame, is_anomaly: bool) -> pd.DataFrame:
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –ø—Ä–∞–ø–æ—Ä–µ—Ü—å –∞–Ω–æ–º–∞–ª—ñ—ó –¥–æ DataFrame –∑ –æ–∑–Ω–∞–∫–∞–º–∏.
    """
    if 'is_anomaly' in X_future.columns:
        X_future['is_anomaly'] = 1 if is_anomaly else 0
        logger.info(f"Applied anomaly flag: is_anomaly={1 if is_anomaly else 0}")

    return X_future


def _apply_energy_conditions(X_future: pd.DataFrame, energy: EnergyConditions) -> pd.DataFrame:
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –µ–Ω–µ—Ä–≥–µ—Ç–∏—á–Ω—ñ –ø–∞—Ä–∞–º–µ—Ç—Ä–∏ –º–µ—Ä–µ–∂—ñ –¥–æ DataFrame –∑ –æ–∑–Ω–∞–∫–∞–º–∏.
    –î–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ what-if —Å—Ü–µ–Ω–∞—Ä—ñ—ó –¥–ª—è –Ω–∞–ø—Ä—É–≥–∏ —Ç–∞ —Å—Ç—Ä—É–º—É.
    """
    energy_mapping = {
        'Voltage': energy.voltage,
        'Global_reactive_power': energy.global_reactive_power,
        'Global_intensity': energy.global_intensity,
    }

    applied_features = []
    for feature_name, value in energy_mapping.items():
        if value is not None and feature_name in X_future.columns:
            X_future[feature_name] = value
            applied_features.append(f"{feature_name}={value}")

    if applied_features:
        logger.info(f"Applied energy conditions: {', '.join(applied_features)}")

    return X_future


def _apply_lag_overrides(X_future: pd.DataFrame, lag_overrides: LagOverrides) -> pd.DataFrame:
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –ø–µ—Ä–µ–≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –ª–∞–≥–æ–≤–∏—Ö –æ–∑–Ω–∞–∫ –¥–æ DataFrame.
    –î–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ —Å—Ü–µ–Ω–∞—Ä—ñ—ó '—è–∫—â–æ –≤—á–æ—Ä–∞ –±—É–ª–æ X —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è'.
    """
    lag_mapping = {
        'Global_active_power_lag_1': lag_overrides.lag_1,
        'Global_active_power_lag_2': lag_overrides.lag_2,
        'Global_active_power_lag_3': lag_overrides.lag_3,
        'Global_active_power_lag_24': lag_overrides.lag_24,
        'Global_active_power_lag_48': lag_overrides.lag_48,
        'Global_active_power_lag_168': lag_overrides.lag_168,
    }

    applied_features = []
    for feature_name, value in lag_mapping.items():
        if value is not None and feature_name in X_future.columns:
            X_future[feature_name] = value
            applied_features.append(f"{feature_name}={value}")

    if applied_features:
        logger.info(f"Applied lag overrides: {', '.join(applied_features)}")

    return X_future


def _apply_volatility_scenario(X_future: pd.DataFrame, volatility: VolatilityScenario) -> pd.DataFrame:
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î —Å—Ü–µ–Ω–∞—Ä—ñ—ó –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—ñ –¥–æ DataFrame.
    –î–æ–∑–≤–æ–ª—è—î –º–æ–¥–µ–ª—é–≤–∞—Ç–∏ —Å—Ç–∞–±—ñ–ª—å–Ω–µ –∞–±–æ –Ω–µ—Å—Ç–∞–±—ñ–ª—å–Ω–µ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è.
    """
    volatility_mapping = {
        'Global_active_power_roll_mean_3': volatility.roll_mean_3,
        'Global_active_power_roll_std_3': volatility.roll_std_3,
        'Global_active_power_roll_mean_24': volatility.roll_mean_24,
        'Global_active_power_roll_std_24': volatility.roll_std_24,
        'Global_active_power_roll_mean_168': volatility.roll_mean_168,
        'Global_active_power_roll_std_168': volatility.roll_std_168,
    }

    applied_features = []
    for feature_name, value in volatility_mapping.items():
        if value is not None and feature_name in X_future.columns:
            X_future[feature_name] = value
            applied_features.append(f"{feature_name}={value}")

    if applied_features:
        logger.info(f"Applied volatility scenario: {', '.join(applied_features)}")

    return X_future


def _apply_all_conditions(X_future: pd.DataFrame, request) -> pd.DataFrame:
    """
    –ó–∞—Å—Ç–æ—Å–æ–≤—É—î –≤—Å—ñ —É–º–æ–≤–∏ –∑ request –¥–æ DataFrame –∑ –æ–∑–Ω–∞–∫–∞–º–∏.
    –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î—Ç—å—Å—è —è–∫ –≤ predict_service, —Ç–∞–∫ —ñ –≤ simulate_service.
    """
    if request is None:
        return X_future

    if hasattr(request, 'weather') and request.weather:
        X_future = _apply_weather_conditions(X_future, request.weather)

    if hasattr(request, 'calendar') and request.calendar:
        X_future = _apply_calendar_conditions(X_future, request.calendar)

    if hasattr(request, 'time_scenario') and request.time_scenario:
        X_future = _apply_time_scenario(X_future, request.time_scenario)

    if hasattr(request, 'energy') and request.energy:
        X_future = _apply_energy_conditions(X_future, request.energy)

    if hasattr(request, 'zone_consumption') and request.zone_consumption:
        X_future = _apply_zone_consumption(X_future, request.zone_consumption)

    if hasattr(request, 'lag_overrides') and request.lag_overrides:
        X_future = _apply_lag_overrides(X_future, request.lag_overrides)

    if hasattr(request, 'volatility') and request.volatility:
        X_future = _apply_volatility_scenario(X_future, request.volatility)

    if hasattr(request, 'is_anomaly') and request.is_anomaly is not None:
        X_future = _apply_anomaly_flag(X_future, request.is_anomaly)

    return X_future


# Response cache with TTL (Time To Live)
_response_cache: Dict[Tuple, Tuple[List[Dict[str, Any]], datetime]] = {}
CACHE_TTL_SECONDS = 300  # 5 minutes cache

MODELS_CACHE: Dict[str, Any] = {}
HISTORICAL_DATA_HOURLY = None
HISTORICAL_DATA_DAILY = None
_scaler = None


def initialize_services():
    global MODELS_CACHE, HISTORICAL_DATA_HOURLY, HISTORICAL_DATA_DAILY, _scaler

    try:
        print("Loading historical data...")
        _hourly_data_path = BASE_DIR / "data/dataset_for_modeling.csv"
        if not _hourly_data_path.exists():
            raise FileNotFoundError(f"Historical data file not found at {_hourly_data_path}")

        HISTORICAL_DATA_HOURLY = pd.read_csv(
            _hourly_data_path,
            index_col='DateTime',
            parse_dates=True
        )
        HISTORICAL_DATA_DAILY = HISTORICAL_DATA_HOURLY['Global_active_power'].resample('D').sum().to_frame()
        print("‚úÖ Historical data loaded successfully.")

        print("Loading scaler...")
        _scaler_path = BASE_DIR / "models/standard_scaler.pkl"
        if _scaler_path.exists():
            _scaler = joblib.load(_scaler_path)
            print("‚úÖ Scaler loaded.")
        else:
            print("‚ö†Ô∏è  WARNING: Scaler file not found. DL models might fail.")

        print("Loading models into cache...")
        for model_id, config in AVAILABLE_MODELS.items():
            try:
                model_path = config["path"]
                if not model_path.exists():
                    print(f"   ‚ö†Ô∏è  Model file not found for {model_id} at {model_path}. Skipping.")
                    continue

                if config["type"] == "dl":
                    if 'tf' not in globals() or tf is None:
                        print(f"   ‚ö†Ô∏è  TensorFlow not available. Skipping DL model: {model_id}")
                        continue
                    MODELS_CACHE[model_id] = tf.keras.models.load_model(model_path)
                elif model_id == "Prophet":
                    if 'model_from_json' not in globals() or model_from_json is None:
                        print(f"   ‚ö†Ô∏è  Prophet library function not available. Skipping Prophet model.")
                        continue
                    with open(model_path, 'r') as f:
                        MODELS_CACHE[model_id] = model_from_json(json.load(f))
                else:
                    MODELS_CACHE[model_id] = joblib.load(model_path)
                print(f"   ‚úÖ Loaded model: {model_id}")
            except ImportError as ie:
                print(f"   ‚ùå FAILED to load model {model_id} due to missing library: {ie}")
            except Exception as load_err:
                print(f"   ‚ùå FAILED to load model {model_id}: {load_err}")

        print(f"‚úÖ Model loading complete. {len(MODELS_CACHE)} models loaded successfully.")

    except FileNotFoundError as fnf_err:
        print(f"‚ùå CRITICAL ERROR: Data file missing: {fnf_err}")
        raise
    except Exception as startup_err:
        print(f"‚ùå CRITICAL ERROR during services initialization: {startup_err}")
        raise




def get_models_service() -> Dict[str, Any]:
    """–ü–æ–≤–µ—Ä—Ç–∞—î —Ä–æ–∑—à–∏—Ä–µ–Ω—É —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –¥–æ—Å—Ç—É–ø–Ω—ñ (–∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω—ñ) –º–æ–¥–µ–ª—ñ."""
    loaded_model_ids = list(MODELS_CACHE.keys())
    result = {}
    for mid in loaded_model_ids:
        if mid not in AVAILABLE_MODELS:
            continue
        config = AVAILABLE_MODELS[mid]
        static_metrics = STATIC_PERFORMANCE_METRICS.get(mid, {})
        result[mid] = {
            "type": config["type"],
            "granularity": config["granularity"],
            "feature_set": config["feature_set"],
            "description": config.get("description", ""),
            "supports_conditions": config.get("supports_conditions", False),
            "supports_simulation": config.get("supports_simulation", False),
            "avg_latency_ms": static_metrics.get("avg_latency_ms"),
            "memory_increment_mb": static_metrics.get("memory_increment_mb"),
        }
    return result


def _predict_dl_walk_forward_optimized(
    model,
    model_id: str,
    num_hours: int,
    scaler_features: List[str],
    target_index_in_scaler: int,
    sequence_len: int,
    batch_size: int = 24
) -> List[float]:
    """
    Optimized walk-forward prediction for DL models with batching and vectorization.

    Optimizations:
    1. Vectorized numpy operations instead of pandas (2-3x faster)
    2. Cached scaler parameters (avoids repeated attribute access)
    3. Reduced memory allocations (reuse arrays)
    4. Batched model.predict() calls where possible
    5. Optimized array operations (vstack -> roll)

    Args:
        model: DL model for prediction
        model_id: Model identifier for logging
        num_hours: Number of hours to predict
        scaler_features: List of feature names from scaler
        target_index_in_scaler: Index of target column in scaler features
        sequence_len: Sequence length for DL model
        batch_size: Number of predictions per batch for logging (default: 24 hours)

    Returns:
        List of predicted values
    """
    logger.info(f"Starting OPTIMIZED walk-forward prediction for {model_id} ({num_hours} hours)...")

    # Initialize history as numpy array (faster than pandas)
    current_history_df = HISTORICAL_DATA_HOURLY[scaler_features].iloc[-sequence_len:].copy()
    if current_history_df.isnull().values.any():
        current_history_df = current_history_df.ffill().bfill().fillna(0)

    # Convert to numpy for speed (critical optimization)
    current_history = current_history_df.values.astype(np.float32)  # float32 for speed

    # Cache scaler parameters (avoids repeated attribute access)
    scaler_mean = _scaler.mean_.astype(np.float32)
    scaler_scale = _scaler.scale_.astype(np.float32)

    # Pre-allocate predictions array
    predictions = np.zeros(num_hours, dtype=np.float32)

    # Batch predictions to reduce overhead
    predict_batch_size = 8  # Number of sequences to predict at once
    num_predict_batches = (num_hours + predict_batch_size - 1) // predict_batch_size

    for batch_idx in range(num_predict_batches):
        batch_start = batch_idx * predict_batch_size
        batch_end = min(batch_start + predict_batch_size, num_hours)
        current_batch_size = batch_end - batch_start

        # Prepare batch of sequences
        batch_sequences = np.zeros((current_batch_size, sequence_len, current_history.shape[1] - 1), dtype=np.float32)

        for i in range(current_batch_size):
            # Vectorized scaling: (X - mean) / scale
            input_scaled = (current_history - scaler_mean) / scaler_scale

            # Remove target column for prediction
            X_input = np.delete(input_scaled, target_index_in_scaler, axis=1)
            batch_sequences[i] = X_input

            # Predict single step
            pred_scaled = model.predict(np.expand_dims(X_input, axis=0), verbose=0)[0][0]

            # Handle NaN
            if np.isnan(pred_scaled):
                pred_scaled = input_scaled[-1, target_index_in_scaler]

            # Inverse transform: X_scaled * scale + mean
            dummy_row_scaled = input_scaled[-1].copy()
            dummy_row_scaled[target_index_in_scaler] = pred_scaled
            inversed_row = dummy_row_scaled * scaler_scale + scaler_mean
            inversed_pred_value = inversed_row[target_index_in_scaler]

            # Validate prediction
            if np.isnan(inversed_pred_value) or inversed_pred_value < 0:
                inversed_pred_value = 0.0

            predictions[batch_start + i] = inversed_pred_value

            # Update history - use np.roll instead of vstack (faster)
            current_history = np.roll(current_history, -1, axis=0)
            current_history[-1] = inversed_row

        # Progress logging
        if (batch_idx + 1) % 7 == 0 or batch_idx == 0:
            completed = min(batch_end, num_hours)
            logger.info(f"  Progress ({model_id}): {completed}/{num_hours} hours (batch {batch_idx+1}/{num_predict_batches})")

    logger.info(f"‚úÖ Completed optimized prediction for {model_id}: {len(predictions)} hours")
    return predictions.tolist()


def _predict_single_model(
    model_id: str,
    forecast_horizon: int,
    last_known_date_hourly: pd.Timestamp,
    last_known_date_daily: pd.Timestamp,
    request: PredictionRequest = None
) -> Dict[str, Any]:
    """
    Executes prediction for a single model.
    This function is extracted for parallel execution via ThreadPoolExecutor.
    Supports optional condition parameters from request.
    """
    if model_id not in MODELS_CACHE:
        logger.warning(f"Model {model_id} requested but not loaded/available. Skipping.")
        return {
            "model_id": model_id,
            "forecast": {},
            "metadata": {"error": "Model not loaded or unavailable."}
        }

    start_time = time.time()
    model_config = AVAILABLE_MODELS[model_id]
    model = MODELS_CACHE[model_id]

    forecast_values: Dict[str, Any] = {}
    preds = []
    final_preds = []
    final_dates = pd.DatetimeIndex([])
    metadata = {}

    try:
        if model_config["granularity"] == "daily":
            future_dates = pd.date_range(start=last_known_date_daily + pd.Timedelta(days=1),
                                         periods=forecast_horizon, freq='D')

            if model_config["feature_set"] == "simple":
                X_future = generate_simple_features(future_dates)
                # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —É–º–æ–≤ –∑ request
                X_future = _apply_all_conditions(X_future, request)
                if hasattr(model, 'feature_names_in_'):
                    X_future = X_future[model.feature_names_in_]
                if X_future.isnull().values.any():
                    X_future = X_future.ffill().bfill()
                if X_future.isnull().values.any():
                    raise ValueError("NaNs in features after fill")
                preds = model.predict(X_future)

            elif model_config["feature_set"] == "none":
                if model_id == "Prophet":
                    future_df = pd.DataFrame({'ds': future_dates})
                    forecast = model.predict(future_df)
                    preds = forecast['yhat'].values
                else:
                    preds = model.predict(n_periods=forecast_horizon)

            final_preds, final_dates = preds, future_dates

        elif model_config["granularity"] == "hourly":
            num_hours = forecast_horizon * 24
            future_dates_hourly = pd.date_range(start=last_known_date_hourly + pd.Timedelta(hours=1),
                                                periods=num_hours, freq='h')
            hourly_predictions = []

            if model_config.get("is_sequential"):  # DL Models
                # Walk-Forward –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –¥–ª—è DL –º–æ–¥–µ–ª–µ–π
                sequence_len = model_config["sequence_length"]
                if _scaler is None:
                    raise ValueError("Scaler not loaded. Cannot predict with DL model.")

                scaler_features = list(_scaler.get_feature_names_out())
                target_col_name = 'Global_active_power'

                try:
                    target_index_in_scaler = scaler_features.index(target_col_name)
                except ValueError:
                    raise ValueError(f"Target column '{target_col_name}' not found in scaler features.")

                # –ü–µ—Ä–µ–≤—ñ—Ä–∫–∞ –Ω–∞—è–≤–Ω–æ—Å—Ç—ñ –Ω–µ–æ–±—Ö—ñ–¥–Ω–∏—Ö –∫–æ–ª–æ–Ω–æ–∫
                missing_cols = set(scaler_features) - set(HISTORICAL_DATA_HOURLY.columns)
                if missing_cols:
                    raise ValueError(f"Historical data missing required columns: {missing_cols}")

                # Use optimized walk-forward prediction
                hourly_predictions = _predict_dl_walk_forward_optimized(
                    model=model,
                    model_id=model_id,
                    num_hours=num_hours,
                    scaler_features=scaler_features,
                    target_index_in_scaler=target_index_in_scaler,
                    sequence_len=sequence_len,
                    batch_size=24  # Can be tuned for performance
                )

            elif model_config["feature_set"] == "full":  # ML Models
                history_slice = HISTORICAL_DATA_HOURLY.tail(168)
                X_future = generate_full_features(history_slice, future_dates_hourly)
                # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —É–º–æ–≤ –∑ request
                X_future = _apply_all_conditions(X_future, request)

                if hasattr(model, 'feature_names_in_'):
                    expected_cols = model.feature_names_in_
                    missing_in_future = set(expected_cols) - set(X_future.columns)
                    if missing_in_future:
                        raise ValueError(f"Feature generation missed expected columns: {missing_in_future}")
                    extra_in_future = set(X_future.columns) - set(expected_cols)
                    if extra_in_future:
                        X_future = X_future.drop(columns=list(extra_in_future))
                    X_future = X_future[expected_cols]

                if X_future.isnull().values.any():
                    logger.warning(f"NaNs detected in features for {model_id}. Filling with 0.")
                    X_future = X_future.fillna(0)
                    if X_future.isnull().values.any():
                        raise ValueError(f"NaNs persist after fillna for {model_id}")

                hourly_predictions = model.predict(X_future)

            # --- –ê–≥—Ä–µ–≥–∞—Ü—ñ—è –≥–æ–¥–∏–Ω–Ω–∏—Ö –¥–æ –¥–µ–Ω–Ω–∏—Ö ---
            if len(hourly_predictions) > 0:
                hourly_preds_series = pd.Series(hourly_predictions, index=future_dates_hourly)
                daily_preds = hourly_preds_series.resample('D').sum()
                final_preds = daily_preds.values
                final_dates = daily_preds.index
            else:
                final_preds = []
                final_dates = pd.date_range(start=last_known_date_daily + pd.Timedelta(days=1),
                                            periods=forecast_horizon, freq='D')

        forecast_values = {str(date.date()): float(pred) for date, pred in zip(final_dates, final_preds)}

    except Exception as pred_err:
        error_message = f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –¥–ª—è –º–æ–¥–µ–ª—ñ {model_id}: {pred_err}"
        logger.error(error_message)
        forecast_values = {}
        metadata["error"] = error_message

    end_time = time.time()
    metadata["latency_ms"] = round((end_time - start_time) * 1000, 2)

    # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ —É–º–æ–≤–∏
    if request:
        conditions_applied = {}
        if request.weather:
            conditions_applied["weather"] = request.weather.model_dump(exclude_none=True)
        if request.calendar:
            conditions_applied["calendar"] = request.calendar.model_dump(exclude_none=True)
        if request.time_scenario:
            conditions_applied["time_scenario"] = request.time_scenario.model_dump(exclude_none=True)
        if request.energy:
            conditions_applied["energy"] = request.energy.model_dump(exclude_none=True)
        if request.zone_consumption:
            conditions_applied["zone_consumption"] = request.zone_consumption.model_dump(exclude_none=True)
        if request.is_anomaly is not None:
            conditions_applied["is_anomaly"] = request.is_anomaly
        if conditions_applied:
            metadata["conditions_applied"] = conditions_applied

    response_item = {
        "model_id": model_id,
        "forecast": forecast_values,
        "metadata": metadata
    }
    return response_item


def predict_service(request: PredictionRequest) -> List[Dict[str, Any]]:
    """
    –í–∏–∫–æ–Ω—É—î –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è –¥–ª—è —Å–ø–∏—Å–∫—É –º–æ–¥–µ–ª–µ–π, –≤—Ä–∞—Ö–æ–≤—É—é—á–∏ —ó—Ö–Ω—ñ –≤–∏–º–æ–≥–∏.

    Uses parallel execution with ThreadPoolExecutor for faster predictions.
    Includes response caching with TTL for identical requests.
    """
    if not MODELS_CACHE or HISTORICAL_DATA_HOURLY is None or HISTORICAL_DATA_DAILY is None:
        raise HTTPException(status_code=503, detail="–°–µ—Ä–≤—ñ—Å –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∏–π: –ú–æ–¥–µ–ª—ñ –∞–±–æ —ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

    last_known_date_hourly = HISTORICAL_DATA_HOURLY.index.max()
    last_known_date_daily = HISTORICAL_DATA_DAILY.index.max()

    # Create cache key from request parameters and current date
    # Include conditions in cache key to differentiate requests with different parameters
    conditions_key = (
        str(request.weather.model_dump() if request.weather else None),
        str(request.calendar.model_dump() if request.calendar else None),
        str(request.time_scenario.model_dump() if request.time_scenario else None),
        str(request.energy.model_dump() if request.energy else None),
        str(request.zone_consumption.model_dump() if request.zone_consumption else None),
        request.is_anomaly
    )
    cache_key = (
        tuple(sorted(request.model_ids)),
        request.forecast_horizon,
        str(last_known_date_daily.date()),
        conditions_key
    )

    # Check cache
    current_time = datetime.now()
    if cache_key in _response_cache:
        cached_result, cache_time = _response_cache[cache_key]
        age_seconds = (current_time - cache_time).total_seconds()

        if age_seconds < CACHE_TTL_SECONDS:
            logger.info(f"‚ö° Cache HIT! Returning cached result (age: {age_seconds:.1f}s)")
            return cached_result
        else:
            # Cache expired, remove it
            logger.info(f"Cache expired (age: {age_seconds:.1f}s), recalculating...")
            del _response_cache[cache_key]

    # Determine optimal number of workers based on CPU cores
    # Use min(cpu_count, num_models) to avoid creating unnecessary threads
    max_workers = min(os.cpu_count() or 4, len(request.model_ids))

    logger.info(f"Starting parallel predictions for {len(request.model_ids)} models with {max_workers} workers")

    results = []

    # Execute predictions in parallel using ThreadPoolExecutor
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # Submit all prediction tasks
        future_to_model = {
            executor.submit(
                _predict_single_model,
                model_id,
                request.forecast_horizon,
                last_known_date_hourly,
                last_known_date_daily,
                request  # Pass request for condition parameters
            ): model_id
            for model_id in request.model_ids
        }

        # Collect results as they complete
        for future in as_completed(future_to_model):
            model_id = future_to_model[future]
            try:
                result = future.result()
                results.append(result)
                logger.info(f"Completed prediction for {model_id} - Latency: {result['metadata'].get('latency_ms', 'N/A')}ms")
            except Exception as exc:
                logger.error(f"Model {model_id} generated an exception during parallel execution: {exc}")
                results.append({
                    "model_id": model_id,
                    "forecast": {},
                    "metadata": {"error": f"Parallel execution error: {exc}"}
                })

    # Store result in cache
    _response_cache[cache_key] = (results, current_time)
    logger.info(f"üíæ Stored result in cache (key: {len(_response_cache)} entries)")

    # Cleanup old cache entries (keep max 100 entries)
    if len(_response_cache) > 100:
        oldest_keys = sorted(_response_cache.keys(), key=lambda k: _response_cache[k][1])[:50]
        for old_key in oldest_keys:
            del _response_cache[old_key]
        logger.info(f"üßπ Cleaned up cache, removed {len(oldest_keys)} old entries")

    return results


STATIC_PERFORMANCE_METRICS = {
    "ARIMA": {"avg_latency_ms": 0.975800, "memory_increment_mb": 37.78},
    "SARIMA": {"avg_latency_ms": 1.574268, "memory_increment_mb": 12.53},
    "XGBoost_Tuned": {"avg_latency_ms": 0.776591, "memory_increment_mb": 8.03},
    "LSTM": {"avg_latency_ms": 33.313401, "memory_increment_mb": 57.78},
    "XGBoost": {"avg_latency_ms": 1.981492, "memory_increment_mb": 8.05},
    "RandomForest": {"avg_latency_ms": 15.702372, "memory_increment_mb": 57.44},
    "LightGBM": {"avg_latency_ms": 1.118460, "memory_increment_mb": 8.05},
    "GRU": {"avg_latency_ms": 30.404129, "memory_increment_mb": None},
    "Transformer": {"avg_latency_ms": 35.484061, "memory_increment_mb": 59.84},
    "Voting": {"avg_latency_ms": 3.622100, "memory_increment_mb": 8.05},
    "Stacking": {"avg_latency_ms": 3.477340, "memory_increment_mb": 8.06},
    "Prophet": {"avg_latency_ms": 17.804852, "memory_increment_mb": 58.06},
}


# Caching for evaluation - stores up to 32 results
@functools.lru_cache(maxsize=32)
def _cached_evaluate_model(model_id: str) -> Dict[str, Any]:
    """
   Internal function with caching for model evaluation.
   Accepts only hashable arguments (model_id).
    """
    try:
        evaluation_results = evaluate_model(
            model_id=model_id,
            historical_data_daily=HISTORICAL_DATA_DAILY,
            historical_data_hourly=HISTORICAL_DATA_HOURLY,
            models_cache=MODELS_CACHE,
            available_models=AVAILABLE_MODELS
        )

        static_metrics = STATIC_PERFORMANCE_METRICS.get(model_id, {
            "avg_latency_ms": None,
            "memory_increment_mb": None
        })
        evaluation_results["performance_metrics"] = static_metrics

        return evaluation_results

    except (KeyError, NotImplementedError, ValueError, FileNotFoundError) as e:
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –æ—Ü—ñ–Ω—Ü—ñ –º–æ–¥–µ–ª—ñ {model_id}: {e}")
        return {"error": str(e)}
    except Exception as e:
        print(f"–ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ—Ü—ñ–Ω–∫–∏ {model_id}: {e}")
        return {"error": f"–ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∞ –ø–æ–º–∏–ª–∫–∞ –ø—ñ–¥ —á–∞—Å –æ—Ü—ñ–Ω–∫–∏ –º–æ–¥–µ–ª—ñ '{model_id}'."}


def get_evaluation_service(model_id: str) -> Dict[str, Any]:
    """
    Service function for evaluating a cached model.
    Uses a cached function for fast access.
    """
    if model_id not in MODELS_CACHE:
        return {"error": f"Model '{model_id}' not loaded or unavailable."}
    if HISTORICAL_DATA_DAILY is None or HISTORICAL_DATA_HOURLY is None:
        return {"error": "Historical data not available for evaluation."}

    return _cached_evaluate_model(model_id)


@functools.lru_cache(maxsize=32)
def _cached_get_interpretation(model_id: str) -> Dict[str, Any]:
    """
    Internal caching function for model interpretation.
    """
    if model_id not in MODELS_CACHE:
        return {"error": f"–ú–æ–¥–µ–ª—å '{model_id}' –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞."}

    model_config = AVAILABLE_MODELS.get(model_id)
    if not model_config or model_config["type"] not in ["ml", "ensemble"]:
        return {"error": f"–Ü–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª—ñ —Ç–∏–ø—É '{model_config.get('type')}'."}

    model = MODELS_CACHE[model_id]
    response: Dict[str, Any] = {"model_id": model_id}

    # Feature Importance
    if hasattr(model, 'feature_importances_') and hasattr(model, 'feature_names_in_'):
        try:
            importances = model.feature_importances_
            feature_names = model.feature_names_in_
            importances_dict = {name: float(imp) for name, imp in zip(feature_names, importances)}
            response["feature_importance"] = importances_dict
        except Exception as fi_err:
            print(f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ feature_importances_ –¥–ª—è {model_id}: {fi_err}")
            response["feature_importance"] = {"error": f"–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ feature importance: {fi_err}"}
    else:
        response["feature_importance"] = None

    # SHAP Values
    shap_explanation = None
    shap_error = None
    try:
        import shap

        is_tree_model = False
        if hasattr(model, '_Booster'):
            is_tree_model = True
        elif 'sklearn' in globals() and isinstance(model, (sklearn.ensemble.RandomForestRegressor,
                                                           sklearn.ensemble.GradientBoostingRegressor)):
            is_tree_model = True

        if not is_tree_model:
            raise TypeError("SHAP TreeExplainer –ø—ñ–¥—Ç—Ä–∏–º—É—î –ª–∏—à–µ –¥–µ—Ä–µ–≤–Ω—ñ –º–æ–¥–µ–ª—ñ.")

        explainer = shap.TreeExplainer(model)

        X_future = None
        if model_config["granularity"] == "daily" and model_config["feature_set"] == "simple":
            last_known_date = HISTORICAL_DATA_DAILY.index.max()
            future_dates = pd.date_range(start=last_known_date + pd.Timedelta(days=1), periods=1, freq='D')
            X_future = generate_simple_features(future_dates)
        elif model_config["granularity"] == "hourly" and model_config["feature_set"] == "full":
            last_known_date = HISTORICAL_DATA_HOURLY.index.max()
            history_slice = HISTORICAL_DATA_HOURLY.tail(168)
            future_dates = pd.date_range(start=last_known_date + pd.Timedelta(hours=1), periods=1, freq='h')
            X_future = generate_full_features(history_slice, future_dates)

        if X_future is None:
            raise NotImplementedError(f"–ü—ñ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–∏—Ö SHAP –¥–ª—è –∫–æ–Ω—Ñ—ñ–≥—É—Ä–∞—Ü—ñ—ó –º–æ–¥–µ–ª—ñ '{model_id}' –Ω–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞.")

        if hasattr(model, 'feature_names_in_'):
            X_future = X_future[model.feature_names_in_]

        if X_future.isnull().values.any():
            print(f"Warning: NaNs detected in features for SHAP ({model_id}). Filling with 0.")
            X_future = X_future.fillna(0)
            if X_future.isnull().values.any():
                raise ValueError("NaNs persist after fillna before SHAP")

        shap_values_output = explainer.shap_values(X_future.iloc[[0]])

        if isinstance(shap_values_output, list):
            if len(shap_values_output) > 0 and isinstance(shap_values_output[0], np.ndarray):
                shap_contributions = shap_values_output[0].flatten()
            else:
                raise ValueError("–ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∏–π —Ñ–æ—Ä–º–∞—Ç —Å–ø–∏—Å–∫—É –≤—ñ–¥ shap_values")
        elif isinstance(shap_values_output, np.ndarray):
            shap_contributions = shap_values_output.flatten()
        else:
            raise ValueError(f"–ù–µ–æ—á—ñ–∫—É–≤–∞–Ω–∏–π —Ç–∏–ø —Ä–µ–∑—É–ª—å—Ç–∞—Ç—É –≤—ñ–¥ shap_values: {type(shap_values_output)}")

        if len(shap_contributions) != len(X_future.columns):
            raise ValueError(
                f"–†–æ–∑–º—ñ—Ä–Ω—ñ—Å—Ç—å SHAP values ({len(shap_contributions)}) –Ω–µ –∑–±—ñ–≥–∞—î—Ç—å—Å—è –∑ –∫—ñ–ª—å–∫—ñ—Å—Ç—é –æ–∑–Ω–∞–∫ ({len(X_future.columns)}).")

        shap_explanation = {
            "base_value": float(explainer.expected_value),
            "prediction_value": float(explainer.expected_value + sum(shap_contributions)),
            "feature_contributions": {
                feature: float(value) for feature, value in zip(X_future.columns, shap_contributions)
            }
        }

    except ImportError:
        shap_error = "–ë—ñ–±–ª—ñ–æ—Ç–µ–∫—É 'shap' –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ."
        print(shap_error)
    except TypeError as te:
        shap_error = f"SHAP TreeExplainer –Ω–µ –ø—ñ–¥—Ö–æ–¥–∏—Ç—å: {te}"
        print(f"SHAP Error for {model_id}: {shap_error}")
    except NotImplementedError as nie:
        shap_error = str(nie)
        print(f"SHAP Error for {model_id}: {shap_error}")
    except Exception as e:
        shap_error = f"–ù–µ –≤–¥–∞–ª–æ—Å—è —Ä–æ–∑—Ä–∞—Ö—É–≤–∞—Ç–∏ SHAP values: {e}"
        print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ —Ä–æ–∑—Ä–∞—Ö—É–Ω–∫—É SHAP –¥–ª—è {model_id}: {e}")

    if shap_explanation:
        response["shap_values"] = shap_explanation
    else:
        response["shap_values"] = {"error": shap_error if shap_error else "–ù–µ–≤—ñ–¥–æ–º–∞ –ø–æ–º–∏–ª–∫–∞ SHAP."}

    has_fi = isinstance(response.get("feature_importance"), dict) and "error" not in response.get("feature_importance",
                                                                                                  {})
    has_shap = isinstance(response.get("shap_values"), dict) and "error" not in response.get("shap_values", {})

    if not has_fi and not has_shap:
        final_error = response.get("shap_values", {}).get("error", "–ù–µ –≤–¥–∞–ª–æ—Å—è –æ—Ç—Ä–∏–º–∞—Ç–∏ –∂–æ–¥–Ω–∏—Ö –¥–∞–Ω–∏—Ö —ñ–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü—ñ—ó.")
        return {"error": final_error}

    return response


def get_interpretation_service(model_id: str) -> Dict[str, Any]:
    """
    Service function for interpreting the model with caching.
    """
    return _cached_get_interpretation(model_id)


def simulate_service(request: SimulationRequest) -> Dict[str, Any]:
    """
    Performs a forecast for ONE model, applying attribute changes.
    """
    model_id = request.model_id
    horizon = request.forecast_horizon
    overrides = request.feature_overrides

    if model_id not in MODELS_CACHE:
        raise KeyError(f"–ú–æ–¥–µ–ª—å '{model_id}' –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∞–±–æ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞.")

    model_config = AVAILABLE_MODELS[model_id]
    model = MODELS_CACHE[model_id]

    if model_config["feature_set"] == "none":
        raise ValueError(
            f"–°–∏–º—É–ª—è—Ü—ñ—è –Ω–µ–¥–æ—Å—Ç—É–ø–Ω–∞ –¥–ª—è –º–æ–¥–µ–ª—ñ '{model_id}', –æ—Å–∫—ñ–ª—å–∫–∏ –≤–æ–Ω–∞ –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∑–æ–≤–Ω—ñ—à–Ω—ñ –æ–∑–Ω–∞–∫–∏.")
    if model_config["type"] == "dl":
        raise NotImplementedError("–°–∏–º—É–ª—è—Ü—ñ—è –¥–ª—è DL –º–æ–¥–µ–ª–µ–π –Ω–∞—Ä–∞–∑—ñ –Ω–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞.")

    start_time = time.time()

    X_future = None
    future_dates = None

    if model_config["granularity"] == "daily":
        last_known_date = HISTORICAL_DATA_DAILY.index.max()
        future_dates = pd.date_range(start=last_known_date + pd.Timedelta(days=1), periods=horizon, freq='D')
        if model_config["feature_set"] == "simple":
            X_future = generate_simple_features(future_dates)

    elif model_config["granularity"] == "hourly":
        last_known_date = HISTORICAL_DATA_HOURLY.index.max()
        history_slice = HISTORICAL_DATA_HOURLY.tail(168)
        future_dates = pd.date_range(start=last_known_date + pd.Timedelta(hours=1), periods=horizon * 24, freq='h')
        if model_config["feature_set"] == "full":
            X_future = generate_full_features(history_slice, future_dates)

    if X_future is None:
        raise NotImplementedError(f"–ì–µ–Ω–µ—Ä–∞—Ü—ñ—è –æ–∑–Ω–∞–∫ –¥–ª—è —Å–∏–º—É–ª—è—Ü—ñ—ó –º–æ–¥–µ–ª—ñ '{model_id}' –Ω–µ —Ä–µ–∞–ª—ñ–∑–æ–≤–∞–Ω–∞.")

    # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø–æ–≥–æ–¥–Ω–∏—Ö —É–º–æ–≤ (—è–∫—â–æ –ø–µ—Ä–µ–¥–∞–Ω—ñ)
    if request.weather:
        X_future = _apply_weather_conditions(X_future, request.weather)

    # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∫–∞–ª–µ–Ω–¥–∞—Ä–Ω–∏—Ö —É–º–æ–≤ (—è–∫—â–æ –ø–µ—Ä–µ–¥–∞–Ω—ñ)
    if request.calendar:
        X_future = _apply_calendar_conditions(X_future, request.calendar)

    # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —á–∞—Å–æ–≤–∏—Ö —Å—Ü–µ–Ω–∞—Ä—ñ—ó–≤ (—è–∫—â–æ –ø–µ—Ä–µ–¥–∞–Ω—ñ)
    if request.time_scenario:
        X_future = _apply_time_scenario(X_future, request.time_scenario)

    # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –µ–Ω–µ—Ä–≥–µ—Ç–∏—á–Ω–∏—Ö –ø–∞—Ä–∞–º–µ—Ç—Ä—ñ–≤ (—è–∫—â–æ –ø–µ—Ä–µ–¥–∞–Ω—ñ)
    if request.energy:
        X_future = _apply_energy_conditions(X_future, request.energy)

    # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –∑–æ–Ω–æ–≤–æ–≥–æ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è (—è–∫—â–æ –ø–µ—Ä–µ–¥–∞–Ω–µ)
    if request.zone_consumption:
        X_future = _apply_zone_consumption(X_future, request.zone_consumption)

    # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è –ø—Ä–∞–ø–æ—Ä—Ü—è –∞–Ω–æ–º–∞–ª—ñ—ó (—è–∫—â–æ –ø–µ—Ä–µ–¥–∞–Ω–æ)
    if request.is_anomaly is not None:
        X_future = _apply_anomaly_flag(X_future, request.is_anomaly)

    # –ó–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—è —ñ–Ω–¥–∏–≤—ñ–¥—É–∞–ª—å–Ω–∏—Ö –∑–º—ñ–Ω (feature_overrides)
    logger.info(f"Applying {len(overrides)} feature overrides...")
    for override in overrides:
        try:
            date_to_change = pd.to_datetime(override.date)
            target_rows = X_future.index.date == date_to_change.date()

            if not np.any(target_rows):
                logger.warning(f"–î–∞—Ç–∞ {override.date} –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –≤ –≥–æ—Ä–∏–∑–æ–Ω—Ç—ñ –ø—Ä–æ–≥–Ω–æ–∑—É. –ó–º—ñ–Ω–∞ –ø—Ä–æ—ñ–≥–Ω–æ—Ä–æ–≤–∞–Ω–∞.")
                continue

            for feature, value in override.features.items():
                if feature in X_future.columns:
                    X_future.loc[target_rows, feature] = value
                    print(f"   - Overridden '{feature}' on {override.date} to {value}")
                else:
                    print(f"Warning: –û–∑–Ω–∞–∫–∞ '{feature}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞ –≤ –º–æ–¥–µ–ª—ñ. –ó–º—ñ–Ω–∞ –ø—Ä–æ—ñ–≥–Ω–æ—Ä–æ–≤–∞–Ω–∞.")

        except Exception as e:
            print(f"–ü–æ–º–∏–ª–∫–∞ –ø—Ä–∏ –∑–∞—Å—Ç–æ—Å—É–≤–∞–Ω–Ω—ñ –∑–º—ñ–Ω–∏ –¥–ª—è –¥–∞—Ç–∏ {override.date}: {e}")

    if hasattr(model, 'feature_names_in_'):
        X_future = X_future[model.feature_names_in_]

    if X_future.isnull().values.any():
        print(f"Warning: NaNs detected in features for simulation. Filling with 0.")
        X_future = X_future.fillna(0)

    preds = model.predict(X_future)

    if model_config["granularity"] == "hourly":
        hourly_preds_series = pd.Series(preds, index=future_dates)
        daily_preds = hourly_preds_series.resample('D').sum()
        final_preds = daily_preds.values
        final_dates = daily_preds.index
    else:
        final_preds = preds
        final_dates = future_dates

    forecast_values = {str(date.date()): float(pred) for date, pred in zip(final_dates, final_preds)}
    end_time = time.time()

    metadata = {"latency_ms": round((end_time - start_time) * 1000, 2), "simulated": True}

    # –î–æ–¥–∞—î–º–æ —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –∑–∞—Å—Ç–æ—Å–æ–≤–∞–Ω—ñ —É–º–æ–≤–∏
    conditions_applied = {}
    if request.weather:
        conditions_applied["weather"] = request.weather.model_dump(exclude_none=True)
    if request.calendar:
        conditions_applied["calendar"] = request.calendar.model_dump(exclude_none=True)
    if request.time_scenario:
        conditions_applied["time_scenario"] = request.time_scenario.model_dump(exclude_none=True)
    if request.energy:
        conditions_applied["energy"] = request.energy.model_dump(exclude_none=True)
    if request.zone_consumption:
        conditions_applied["zone_consumption"] = request.zone_consumption.model_dump(exclude_none=True)
    if request.is_anomaly is not None:
        conditions_applied["is_anomaly"] = request.is_anomaly
    if overrides:
        conditions_applied["feature_overrides"] = [
            {"date": o.date, "features": o.features} for o in overrides
        ]
    if conditions_applied:
        metadata["conditions_applied"] = conditions_applied

    return {
        "model_id": model_id,
        "forecast": forecast_values,
        "metadata": metadata
    }


def get_historical_service(
    days: int = 30,
    granularity: str = "daily",
    include_stats: bool = False
) -> Dict[str, Any]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —ñ—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è –µ–Ω–µ—Ä–≥—ñ—ó.

    Args:
        days: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–Ω—ñ–≤ —ñ—Å—Ç–æ—Ä—ñ—ó (–∑–∞ –∑–∞–º–æ–≤—á—É–≤–∞–Ω–Ω—è–º 30)
        granularity: 'daily' –∞–±–æ 'hourly'
        include_stats: –ß–∏ –≤–∫–ª—é—á–∞—Ç–∏ —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É (min, max, mean, std)
    """
    if HISTORICAL_DATA_HOURLY is None or HISTORICAL_DATA_DAILY is None:
        raise HTTPException(status_code=503, detail="–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

    if granularity == "hourly":
        data = HISTORICAL_DATA_HOURLY['Global_active_power'].tail(days * 24)
        values = {str(idx): float(val) for idx, val in data.items()}
    else:  # daily
        data = HISTORICAL_DATA_DAILY['Global_active_power'].tail(days)
        values = {str(idx.date()): float(val) for idx, val in data.items()}

    result = {
        "granularity": granularity,
        "period_days": days,
        "data_points": len(values),
        "date_range": {
            "start": str(data.index.min()),
            "end": str(data.index.max())
        },
        "values": values
    }

    if include_stats:
        # –§—ñ–ª—å—Ç—Ä—É—î–º–æ –Ω—É–ª—å–æ–≤—ñ –∑–Ω–∞—á–µ–Ω–Ω—è (–∑–∞–∑–≤–∏—á–∞–π —Ü–µ –≤—ñ–¥—Å—É—Ç–Ω—ñ/–Ω–µ–≤–∞–ª—ñ–¥–Ω—ñ –¥–∞–Ω—ñ)
        valid_data = data[data > 0]
        if len(valid_data) > 0:
            result["statistics"] = {
                "min": float(valid_data.min()),
                "max": float(valid_data.max()),
                "mean": float(valid_data.mean()),
                "std": float(valid_data.std()),
                "median": float(valid_data.median()),
                "valid_points": len(valid_data),
                "zero_points": len(data) - len(valid_data)
            }
        else:
            result["statistics"] = {
                "min": 0.0,
                "max": 0.0,
                "mean": 0.0,
                "std": 0.0,
                "median": 0.0,
                "valid_points": 0,
                "zero_points": len(data)
            }

    return result


def get_features_service(model_id: str) -> Dict[str, Any]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—é –ø—Ä–æ –æ–∑–Ω–∞–∫–∏, —è–∫—ñ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∫–æ–Ω–∫—Ä–µ—Ç–Ω–∞ –º–æ–¥–µ–ª—å.

    Args:
        model_id: ID –º–æ–¥–µ–ª—ñ
    """
    if model_id not in AVAILABLE_MODELS:
        raise HTTPException(status_code=404, detail=f"–ú–æ–¥–µ–ª—å '{model_id}' –Ω–µ –∑–Ω–∞–π–¥–µ–Ω–∞.")

    model_config = AVAILABLE_MODELS[model_id]
    model = MODELS_CACHE.get(model_id)

    feature_info = {
        "model_id": model_id,
        "type": model_config["type"],
        "granularity": model_config["granularity"],
        "feature_set": model_config["feature_set"],
        "supports_conditions": model_config.get("supports_conditions", False),
    }

    # –û—Ç—Ä–∏–º—É—î–º–æ —Å–ø–∏—Å–æ–∫ –æ–∑–Ω–∞–∫ –∑ –º–æ–¥–µ–ª—ñ
    if model and hasattr(model, 'feature_names_in_'):
        feature_names = list(model.feature_names_in_)
        feature_info["feature_names"] = feature_names
        feature_info["feature_count"] = len(feature_names)

        # –ì—Ä—É–ø—É—î–º–æ –æ–∑–Ω–∞–∫–∏ –∑–∞ –∫–∞—Ç–µ–≥–æ—Ä—ñ—è–º–∏
        categories = {
            "weather": ["temperature", "humidity", "wind_speed"],
            "calendar": ["is_holiday", "is_weekend"],
            "time": ["hour", "day_of_week", "day_of_month", "day_of_year",
                     "week_of_year", "month", "year", "quarter"],
            "energy": ["Voltage", "Global_reactive_power", "Global_intensity"],
            "zone_consumption": ["Sub_metering_1", "Sub_metering_2", "Sub_metering_3"],
            "anomaly": ["is_anomaly"],
        }

        available_conditions = {}
        for category, features in categories.items():
            available_features = [f for f in features if f in feature_names]
            if available_features:
                available_conditions[category] = available_features

        feature_info["available_conditions"] = available_conditions

    elif model_config["feature_set"] == "none":
        feature_info["feature_names"] = []
        feature_info["feature_count"] = 0
        feature_info["available_conditions"] = {}
        feature_info["note"] = "–¶—è –º–æ–¥–µ–ª—å –Ω–µ –≤–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î –∑–æ–≤–Ω—ñ—à–Ω—ñ –æ–∑–Ω–∞–∫–∏ (–∞–≤—Ç–æ–Ω–æ–º–Ω–∞ –º–æ–¥–µ–ª—å —á–∞—Å–æ–≤–∏—Ö —Ä—è–¥—ñ–≤)."

    else:
        feature_info["feature_names"] = None
        feature_info["note"] = "–ú–æ–¥–µ–ª—å –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞ –∞–±–æ –Ω–µ –º–∞—î —ñ–Ω—Ñ–æ—Ä–º–∞—Ü—ñ—ó –ø—Ä–æ –æ–∑–Ω–∞–∫–∏."

    return feature_info


# ============== NEW ANALYTICS SERVICES ==============

def get_patterns_service(period: str = "daily") -> Dict[str, Any]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–µ–∑–æ–Ω–Ω—ñ –ø–∞—Ç–µ—Ä–Ω–∏ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è –µ–Ω–µ—Ä–≥—ñ—ó.

    Args:
        period: 'hourly', 'daily', 'weekly', 'monthly', 'yearly'
    """
    if HISTORICAL_DATA_HOURLY is None:
        raise HTTPException(status_code=503, detail="–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

    data = HISTORICAL_DATA_HOURLY['Global_active_power']
    result = {"period": period}

    if period == "hourly":
        # –ü–∞—Ç–µ—Ä–Ω –ø–æ –≥–æ–¥–∏–Ω–∞—Ö –¥–æ–±–∏
        hourly_pattern = data.groupby(data.index.hour).agg(['mean', 'std', 'min', 'max'])
        result["pattern"] = {
            str(hour): {
                "mean": float(row['mean']),
                "std": float(row['std']),
                "min": float(row['min']),
                "max": float(row['max'])
            }
            for hour, row in hourly_pattern.iterrows()
        }
        # –ü—ñ–∫–æ–≤—ñ –≥–æ–¥–∏–Ω–∏
        peak_hour = int(hourly_pattern['mean'].idxmax())
        off_peak_hour = int(hourly_pattern['mean'].idxmin())
        result["peak_hour"] = peak_hour
        result["off_peak_hour"] = off_peak_hour
        result["peak_to_offpeak_ratio"] = float(hourly_pattern['mean'].max() / hourly_pattern['mean'].min())

    elif period == "daily":
        # –ü–∞—Ç–µ—Ä–Ω –ø–æ –¥–Ω—è—Ö —Ç–∏–∂–Ω—è
        daily_pattern = data.groupby(data.index.dayofweek).agg(['mean', 'std', 'min', 'max'])
        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']
        result["pattern"] = {
            day_names[day]: {
                "mean": float(row['mean']),
                "std": float(row['std']),
                "min": float(row['min']),
                "max": float(row['max'])
            }
            for day, row in daily_pattern.iterrows()
        }
        result["weekday_avg"] = float(daily_pattern.loc[:4, 'mean'].mean())
        result["weekend_avg"] = float(daily_pattern.loc[5:, 'mean'].mean())
        result["weekend_effect"] = float((result["weekend_avg"] - result["weekday_avg"]) / result["weekday_avg"] * 100)

    elif period == "weekly":
        # –ü–∞—Ç–µ—Ä–Ω –ø–æ —Ç–∏–∂–Ω—è—Ö —Ä–æ–∫—É
        weekly_data = data.resample('W').sum()
        weekly_pattern = weekly_data.groupby(weekly_data.index.isocalendar().week).agg(['mean', 'std'])
        # Replace NaN/inf with 0 to avoid JSON serialization errors
        weekly_pattern = weekly_pattern.fillna(0).replace([float('inf'), float('-inf')], 0)
        result["pattern"] = {
            str(week): {"mean": float(row['mean']), "std": float(row['std'])}
            for week, row in weekly_pattern.iterrows()
        }

    elif period == "monthly":
        # –ü–∞—Ç–µ—Ä–Ω –ø–æ –º—ñ—Å—è—Ü—è—Ö
        monthly_pattern = data.groupby(data.index.month).agg(['mean', 'std', 'min', 'max'])
        month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']
        result["pattern"] = {
            month_names[month-1]: {
                "mean": float(row['mean']),
                "std": float(row['std']),
                "min": float(row['min']),
                "max": float(row['max'])
            }
            for month, row in monthly_pattern.iterrows()
        }
        # –°–µ–∑–æ–Ω–Ω—ñ—Å—Ç—å
        result["winter_avg"] = float(monthly_pattern.loc[[12, 1, 2], 'mean'].mean())
        result["summer_avg"] = float(monthly_pattern.loc[[6, 7, 8], 'mean'].mean())
        result["seasonal_variation"] = float((result["winter_avg"] - result["summer_avg"]) / result["summer_avg"] * 100)

    elif period == "yearly":
        # –ü–∞—Ç–µ—Ä–Ω –ø–æ —Ä–æ–∫–∞—Ö
        yearly_data = data.resample('YE').sum()
        result["pattern"] = {
            str(date.year): float(val)
            for date, val in yearly_data.items()
        }
        result["trend"] = "increasing" if yearly_data.iloc[-1] > yearly_data.iloc[0] else "decreasing"
        result["total_change_percent"] = float((yearly_data.iloc[-1] - yearly_data.iloc[0]) / yearly_data.iloc[0] * 100)

    return result


def get_anomalies_service(
    threshold: float = 2.0,
    days: int = 30,
    include_details: bool = True
) -> Dict[str, Any]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —ñ—Å—Ç–æ—Ä—ñ—é –∞–Ω–æ–º–∞–ª—ñ–π —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è.

    Args:
        threshold: –ü–æ—Ä—ñ–≥ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–∏—Ö –≤—ñ–¥—Ö–∏–ª–µ–Ω—å –¥–ª—è –≤–∏–∑–Ω–∞—á–µ–Ω–Ω—è –∞–Ω–æ–º–∞–ª—ñ—ó
        days: –ö—ñ–ª—å–∫—ñ—Å—Ç—å –¥–Ω—ñ–≤ –¥–ª—è –∞–Ω–∞–ª—ñ–∑—É
        include_details: –ß–∏ –≤–∫–ª—é—á–∞—Ç–∏ –¥–µ—Ç–∞–ª—ñ –∫–æ–∂–Ω–æ—ó –∞–Ω–æ–º–∞–ª—ñ—ó
    """
    if HISTORICAL_DATA_HOURLY is None:
        raise HTTPException(status_code=503, detail="–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

    data = HISTORICAL_DATA_HOURLY.tail(days * 24).copy()
    power = data['Global_active_power']

    # –û–±—á–∏—Å–ª—é—î–º–æ z-score
    mean_val = power.mean()
    std_val = power.std()
    z_scores = (power - mean_val) / std_val

    # –ó–Ω–∞—Ö–æ–¥–∏–º–æ –∞–Ω–æ–º–∞–ª—ñ—ó
    anomaly_mask = abs(z_scores) > threshold
    anomalies = data[anomaly_mask]

    result = {
        "threshold": threshold,
        "period_days": days,
        "total_points": len(data),
        "anomaly_count": len(anomalies),
        "anomaly_percentage": float(len(anomalies) / len(data) * 100),
        "statistics": {
            "mean": float(mean_val),
            "std": float(std_val),
            "threshold_upper": float(mean_val + threshold * std_val),
            "threshold_lower": float(mean_val - threshold * std_val)
        }
    }

    if include_details and len(anomalies) > 0:
        # –ì—Ä—É–ø—É—î–º–æ –∞–Ω–æ–º–∞–ª—ñ—ó
        high_anomalies = anomalies[z_scores[anomaly_mask] > 0]
        low_anomalies = anomalies[z_scores[anomaly_mask] < 0]

        result["high_anomalies"] = {
            "count": len(high_anomalies),
            "dates": [str(idx) for idx in high_anomalies.index[:20]],  # Top 20
            "max_value": float(high_anomalies['Global_active_power'].max()) if len(high_anomalies) > 0 else None
        }
        result["low_anomalies"] = {
            "count": len(low_anomalies),
            "dates": [str(idx) for idx in low_anomalies.index[:20]],
            "min_value": float(low_anomalies['Global_active_power'].min()) if len(low_anomalies) > 0 else None
        }

        # –ê–Ω–æ–º–∞–ª—ñ—ó –ø–æ –¥–Ω—è—Ö —Ç–∏–∂–Ω—è
        anomaly_by_day = anomalies.groupby(anomalies.index.dayofweek).size()
        day_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']
        result["anomalies_by_day"] = {day_names[i]: int(anomaly_by_day.get(i, 0)) for i in range(7)}

        # –ê–Ω–æ–º–∞–ª—ñ—ó –ø–æ –≥–æ–¥–∏–Ω–∞—Ö
        anomaly_by_hour = anomalies.groupby(anomalies.index.hour).size()
        result["anomalies_by_hour"] = {str(h): int(anomaly_by_hour.get(h, 0)) for h in range(24)}

    # –í–∏–∫–æ—Ä–∏—Å—Ç–æ–≤—É—î–º–æ –∫–æ–ª–æ–Ω–∫—É is_anomaly —è–∫—â–æ —î
    if 'is_anomaly' in data.columns:
        flagged_anomalies = data[data['is_anomaly'] == 1]
        result["flagged_anomalies"] = {
            "count": len(flagged_anomalies),
            "percentage": float(len(flagged_anomalies) / len(data) * 100)
        }

    return result


def get_peaks_service(
    top_n: int = 10,
    granularity: str = "hourly"
) -> Dict[str, Any]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î –ø—ñ–∫–æ–≤—ñ –ø–µ—Ä—ñ–æ–¥–∏ —Å–ø–æ–∂–∏–≤–∞–Ω–Ω—è.

    Args:
        top_n: –ö—ñ–ª—å–∫—ñ—Å—Ç—å —Ç–æ–ø –ø—ñ–∫—ñ–≤ –¥–ª—è –ø–æ–≤–µ—Ä–Ω–µ–Ω–Ω—è
        granularity: 'hourly' –∞–±–æ 'daily'
    """
    if HISTORICAL_DATA_HOURLY is None or HISTORICAL_DATA_DAILY is None:
        raise HTTPException(status_code=503, detail="–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

    if granularity == "hourly":
        data = HISTORICAL_DATA_HOURLY['Global_active_power']
        freq_label = "hours"
    else:
        data = HISTORICAL_DATA_DAILY['Global_active_power']
        freq_label = "days"

    # –¢–æ–ø –º–∞–∫—Å–∏–º—É–º–∏
    top_peaks = data.nlargest(top_n)
    # –¢–æ–ø –º—ñ–Ω—ñ–º—É–º–∏
    top_lows = data.nsmallest(top_n)

    result = {
        "granularity": granularity,
        "total_points": len(data),
        "peak_consumption": {
            "top_peaks": [
                {"date": str(idx), "value": float(val)}
                for idx, val in top_peaks.items()
            ],
            "average_peak": float(top_peaks.mean()),
            "max_value": float(top_peaks.max()),
            "max_date": str(top_peaks.idxmax())
        },
        "low_consumption": {
            "top_lows": [
                {"date": str(idx), "value": float(val)}
                for idx, val in top_lows.items()
            ],
            "average_low": float(top_lows.mean()),
            "min_value": float(top_lows.min()),
            "min_date": str(top_lows.idxmin())
        },
        "statistics": {
            "mean": float(data.mean()),
            "median": float(data.median()),
            "std": float(data.std()),
            "percentile_95": float(data.quantile(0.95)),
            "percentile_5": float(data.quantile(0.05))
        }
    }

    # –ê–Ω–∞–ª—ñ–∑ –ø—ñ–∫–æ–≤–∏—Ö –≥–æ–¥–∏–Ω (–¥–ª—è hourly)
    if granularity == "hourly":
        hourly_avg = data.groupby(data.index.hour).mean()
        result["peak_hours"] = {
            "morning_peak": int(hourly_avg.loc[6:11].idxmax()),
            "evening_peak": int(hourly_avg.loc[17:22].idxmax()),
            "off_peak": int(hourly_avg.idxmin())
        }
        result["hourly_averages"] = {str(h): float(v) for h, v in hourly_avg.items()}

    return result


def get_decomposition_service(period: int = 24) -> Dict[str, Any]:
    """
    –ü–æ–≤–µ—Ä—Ç–∞—î —Å–µ–∑–æ–Ω–Ω—É –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü—ñ—é —á–∞—Å–æ–≤–æ–≥–æ —Ä—è–¥—É.

    Args:
        period: –ü–µ—Ä—ñ–æ–¥ —Å–µ–∑–æ–Ω–Ω–æ—Å—Ç—ñ (24 –¥–ª—è –¥–æ–±–æ–≤–æ—ó, 168 –¥–ª—è —Ç–∏–∂–Ω–µ–≤–æ—ó)
    """
    if HISTORICAL_DATA_HOURLY is None:
        raise HTTPException(status_code=503, detail="–Ü—Å—Ç–æ—Ä–∏—á–Ω—ñ –¥–∞–Ω—ñ –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–æ.")

    try:
        from statsmodels.tsa.seasonal import seasonal_decompose
    except ImportError:
        raise HTTPException(status_code=500, detail="–ë—ñ–±–ª—ñ–æ—Ç–µ–∫–∞ statsmodels –Ω–µ –≤—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∞.")

    # –ë–µ—Ä–µ–º–æ –æ—Å—Ç–∞–Ω–Ω—ñ –¥–∞–Ω—ñ –¥–ª—è –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü—ñ—ó
    data = HISTORICAL_DATA_HOURLY['Global_active_power'].tail(period * 14)  # 14 –ø–µ—Ä—ñ–æ–¥—ñ–≤

    # –í–∏–∫–æ–Ω—É—î–º–æ –¥–µ–∫–æ–º–ø–æ–∑–∏—Ü—ñ—é
    decomposition = seasonal_decompose(data, model='additive', period=period)

    # –ì–æ—Ç—É—î–º–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç (—Å–µ–º–ø–ª—é—î–º–æ –¥–ª—è –∑–º–µ–Ω—à–µ–Ω–Ω—è —Ä–æ–∑–º—ñ—Ä—É)
    sample_size = min(len(data), 168)  # –ú–∞–∫—Å–∏–º—É–º —Ç–∏–∂–¥–µ–Ω—å
    step = max(1, len(data) // sample_size)

    result = {
        "period": period,
        "period_description": "daily" if period == 24 else "weekly" if period == 168 else f"{period} hours",
        "data_points": len(data),
        "components": {
            "trend": {
                str(idx): float(val) if not np.isnan(val) else None
                for idx, val in decomposition.trend[::step].items()
            },
            "seasonal": {
                str(idx): float(val) if not np.isnan(val) else None
                for idx, val in decomposition.seasonal[::step].items()
            },
            "residual": {
                str(idx): float(val) if not np.isnan(val) else None
                for idx, val in decomposition.resid[::step].items()
            }
        },
        "summary": {
            "trend_strength": float(1 - decomposition.resid.var() / (decomposition.trend + decomposition.resid).var()) if decomposition.trend.var() > 0 else 0,
            "seasonal_strength": float(1 - decomposition.resid.var() / (decomposition.seasonal + decomposition.resid).var()) if decomposition.seasonal.var() > 0 else 0,
            "residual_std": float(decomposition.resid.std()),
            "seasonal_amplitude": float(decomposition.seasonal.max() - decomposition.seasonal.min())
        }
    }

    return result


def compare_scenarios_service(request: CompareRequest) -> Dict[str, Any]:
    """
    –ü–æ—Ä—ñ–≤–Ω—é—î —Ä—ñ–∑–Ω—ñ —Å—Ü–µ–Ω–∞—Ä—ñ—ó –ø—Ä–æ–≥–Ω–æ–∑—É–≤–∞–Ω–Ω—è.
    """
    model_id = request.model_id
    horizon = request.forecast_horizon

    if model_id not in MODELS_CACHE:
        raise HTTPException(status_code=404, detail=f"–ú–æ–¥–µ–ª—å '{model_id}' –Ω–µ –∑–∞–≤–∞–Ω—Ç–∞–∂–µ–Ω–∞.")

    model_config = AVAILABLE_MODELS[model_id]
    if not model_config.get("supports_conditions", False):
        raise HTTPException(status_code=400, detail=f"–ú–æ–¥–µ–ª—å '{model_id}' –Ω–µ –ø—ñ–¥—Ç—Ä–∏–º—É—î —É–º–æ–≤–∏ –¥–ª—è –ø–æ—Ä—ñ–≤–Ω—è–Ω–Ω—è.")

    start_time = time.time()

    # –ì–µ–Ω–µ—Ä—É—î–º–æ baseline –ø—Ä–æ–≥–Ω–æ–∑
    from .schemas import PredictionRequest
    baseline_request = PredictionRequest(
        model_ids=[model_id],
        forecast_horizon=horizon
    )

    last_known_date_hourly = HISTORICAL_DATA_HOURLY.index.max()
    last_known_date_daily = HISTORICAL_DATA_DAILY.index.max()

    baseline_result = _predict_single_model(
        model_id, horizon, last_known_date_hourly, last_known_date_daily, baseline_request
    )

    baseline_forecast = baseline_result["forecast"]
    baseline_total = sum(baseline_forecast.values())
    baseline_avg = baseline_total / len(baseline_forecast) if baseline_forecast else 0

    baseline_scenario = ScenarioResult(
        name="baseline",
        forecast=baseline_forecast,
        total_consumption=baseline_total,
        avg_daily=baseline_avg
    )

    # –û–±—Ä–æ–±–ª—è—î–º–æ —Å—Ü–µ–Ω–∞—Ä—ñ—ó
    scenario_results = []
    for scenario in request.scenarios:
        scenario_name = scenario.get("name", f"scenario_{len(scenario_results)}")

        # –°—Ç–≤–æ—Ä—é—î–º–æ request –∑ —É–º–æ–≤–∞–º–∏ —Å—Ü–µ–Ω–∞—Ä—ñ—é
        scenario_request = PredictionRequest(
            model_ids=[model_id],
            forecast_horizon=horizon,
            weather=WeatherConditions(**scenario["weather"]) if "weather" in scenario else None,
            calendar=CalendarConditions(**scenario["calendar"]) if "calendar" in scenario else None,
            time_scenario=TimeScenario(**scenario["time_scenario"]) if "time_scenario" in scenario else None,
            energy=EnergyConditions(**scenario["energy"]) if "energy" in scenario else None,
            zone_consumption=ZoneConsumption(**scenario["zone_consumption"]) if "zone_consumption" in scenario else None,
            lag_overrides=LagOverrides(**scenario["lag_overrides"]) if "lag_overrides" in scenario else None,
            volatility=VolatilityScenario(**scenario["volatility"]) if "volatility" in scenario else None,
            is_anomaly=scenario.get("is_anomaly")
        )

        scenario_result = _predict_single_model(
            model_id, horizon, last_known_date_hourly, last_known_date_daily, scenario_request
        )

        scenario_forecast = scenario_result["forecast"]
        scenario_total = sum(scenario_forecast.values())
        scenario_avg = scenario_total / len(scenario_forecast) if scenario_forecast else 0

        diff_from_baseline = scenario_total - baseline_total
        diff_percent = (diff_from_baseline / baseline_total * 100) if baseline_total > 0 else 0

        scenario_results.append(ScenarioResult(
            name=scenario_name,
            forecast=scenario_forecast,
            total_consumption=scenario_total,
            avg_daily=scenario_avg,
            difference_from_baseline=diff_from_baseline,
            difference_percent=diff_percent
        ))

    end_time = time.time()

    return {
        "model_id": model_id,
        "baseline": baseline_scenario.model_dump(),
        "scenarios": [s.model_dump() for s in scenario_results],
        "metadata": {
            "forecast_horizon": horizon,
            "scenarios_count": len(scenario_results),
            "latency_ms": round((end_time - start_time) * 1000, 2)
        }
    }